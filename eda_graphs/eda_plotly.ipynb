{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ad8693cc",
      "metadata": {},
      "source": [
        "# 📰 News Article Exploratory Data Analysis (EDA)\n",
        "*Dataset: 60,000 Company News Articles*\n",
        "\n",
        "This notebook provides a comprehensive, interactive EDA of a large news article dataset using Plotly and pandas.  \n",
        "It covers time trends, topic tags, co-occurrence, velocity, and basic text statistics.\n",
        "\n",
        "**Interactivity:** All visualizations use Plotly—hover, zoom, and export are available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "274dae49",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install plotly if needed (uncomment for Colab)\n",
        "!pip install plotly\n",
        "\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.ticker as ticker\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from itertools import combinations\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class Analyzer:\n",
        "    def __init__(self, source_file):\n",
        "        self.source_file = source_file\n",
        "        self.df = None\n",
        "        self._load_data()\n",
        "        self.tags = set()\n",
        "        self.df[\"date\"] = pd.to_datetime(\n",
        "            self.df[\"CreateDateString\"], format=\"%d.%m.%Y\", errors=\"coerce\"\n",
        "        )\n",
        "\n",
        "    # DF Analysis\n",
        "    def _load_data(self):\n",
        "        \"\"\"Load the JSONL metadata file into a DataFrame.\"\"\"\n",
        "        try:\n",
        "            self.df = pd.read_json(self.source_file, lines=True)\n",
        "            self.df[\"date\"] = pd.to_datetime(\n",
        "                self.df[\"CreateDateString\"], format=\"%d.%m.%Y\"\n",
        "            )\n",
        "\n",
        "            print(f\"✅ Loaded {len(self.df)} records from {self.source_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to load data: {e}\")\n",
        "            self.df = pd.DataFrame()  # Empty fallback\n",
        "\n",
        "    def view_df(self, n=5):\n",
        "        \"\"\"Print the first n rows of the DataFrame.\"\"\"\n",
        "        print(self.df.head(n))\n",
        "\n",
        "    def info(self):\n",
        "        \"\"\"Print summary info (columns, types, non-nulls, etc).\"\"\"\n",
        "        print(self.df.info())\n",
        "\n",
        "    def shape(self):\n",
        "        \"\"\"Return DataFrame shape (rows, columns).\"\"\"\n",
        "        print(self.df.shape)\n",
        "\n",
        "    def show_columns(self):\n",
        "        \"\"\"Print the columns in the DataFrame.\"\"\"\n",
        "        print(self.df.columns.tolist())\n",
        "\n",
        "    def preview_random(self, n=5):\n",
        "        \"\"\"Show a random sample of rows.\"\"\"\n",
        "        print(self.df.sample(n))\n",
        "\n",
        "    def describe_dates(self, date_col=\"CreateDateString\"):\n",
        "        \"\"\"Print the min/max date (if available).\"\"\"\n",
        "        if date_col in self.df.columns:\n",
        "            print(\"Earliest date:\", self.df[date_col].min())\n",
        "            print(\"Latest date:\", self.df[date_col].max())\n",
        "        else:\n",
        "            print(f\"No '{date_col}' column found.\")\n",
        "\n",
        "    def missing_report(self):\n",
        "        \"\"\"Show count of missing values per column.\"\"\"\n",
        "        print(self.df.isnull().sum())\n",
        "\n",
        "    def display_col(self, col):\n",
        "        for value in self.df[col]:\n",
        "            print(value)\n",
        "\n",
        "    # Tag Analysis\n",
        "    def get_tag_counts(self, col=\"Tags\"):\n",
        "        tag_counter = Counter()\n",
        "        for tag_list in self.df[col]:\n",
        "            if isinstance(tag_list, list):\n",
        "                for tag in tag_list:\n",
        "                    if tag:  # skip empty\n",
        "                        tag_counter.update([tag])\n",
        "            elif isinstance(tag_list, str):\n",
        "                # Assume tags in string are comma-separated, or just one tag\n",
        "                tags = [\n",
        "                    tag.strip().lower() for tag in tag_list.split(\",\") if tag.strip()\n",
        "                ]\n",
        "                tag_counter.update(tags)\n",
        "            # Else: skip (could be None, float, etc.)\n",
        "        return tag_counter\n",
        "\n",
        "    def plot_tag_counts(self, tags_col=\"Tags\", top_n=20):\n",
        "        tag_counter = self.get_tag_counts(tags_col)\n",
        "        if not tag_counter:\n",
        "            print(\"No tags found.\")\n",
        "            return\n",
        "        common_tags = tag_counter.most_common(top_n)\n",
        "        tags, counts = zip(*common_tags)\n",
        "        fig = px.bar(\n",
        "            x=list(counts),\n",
        "            y=list(tags),\n",
        "            orientation=\"h\",\n",
        "            labels={\"x\": \"Count\", \"y\": \"Tag\"},\n",
        "            color=list(counts),\n",
        "            color_continuous_scale=\"teal\",\n",
        "            title=f\"Top {top_n} Tags\",\n",
        "        )\n",
        "        fig.update_layout(\n",
        "            yaxis=dict(categoryorder=\"total ascending\"),\n",
        "            height=max(400, int(top_n * 24)),\n",
        "        )\n",
        "        # fig.show()\n",
        "        return fig\n",
        "\n",
        "    # Date Analysis\n",
        "    def articles_per_day(self, print_bool=False):\n",
        "        counts = self.df[\"date\"].value_counts().sort_index()\n",
        "        # print(counts)\n",
        "        if print_bool:\n",
        "            print(counts)\n",
        "        return counts\n",
        "\n",
        "    def articles_per_week(self):\n",
        "        # Group by year-week\n",
        "        weekly = self.df.groupby(self.df[\"date\"].dt.isocalendar().week)[\"Id\"].count()\n",
        "        print(weekly)\n",
        "        return weekly\n",
        "\n",
        "    def articles_per_month(self):\n",
        "        monthly = self.df.groupby(self.df[\"date\"].dt.to_period(\"M\"))[\"Id\"].count()\n",
        "        print(monthly)\n",
        "        return monthly\n",
        "\n",
        "    def longest_shortest_day(self):\n",
        "        counts = self.articles_per_day()\n",
        "        max_count = counts.max()\n",
        "        min_count = counts.min()\n",
        "        busiest = counts[counts == max_count]\n",
        "        slowest = counts[counts == min_count]\n",
        "        busiest_single = busiest.index[0]\n",
        "        slowest_single = slowest.index[0]\n",
        "        print(f\"Most articles: {max_count} on {busiest_single}\")\n",
        "        print(f\"Fewest articles: {min_count} on {slowest_single}\")\n",
        "        return busiest, slowest\n",
        "\n",
        "    def articles_per_month_around_date(\n",
        "        self, selected_date, months_window=3, date_col=\"date\"\n",
        "    ):\n",
        "        if not isinstance(selected_date, pd.Timestamp):\n",
        "            selected_date = pd.to_datetime(selected_date)\n",
        "        start = selected_date - pd.DateOffset(months=months_window)\n",
        "        end = selected_date + pd.DateOffset(months=months_window)\n",
        "        mask = (self.df[date_col] >= start) & (self.df[date_col] <= end)\n",
        "        df_window = self.df[mask]\n",
        "        monthly = df_window.groupby(df_window[date_col].dt.to_period(\"M\"))[\"Id\"].count()\n",
        "        monthly = monthly.reset_index()\n",
        "        monthly[date_col] = monthly[date_col].astype(str)\n",
        "\n",
        "        fig = px.bar(\n",
        "            monthly,\n",
        "            x=date_col,\n",
        "            y=\"Id\",\n",
        "            color=date_col,\n",
        "            color_discrete_sequence=px.colors.sequential.Teal,\n",
        "            labels={\"Id\": \"Article Count\", date_col: \"Month\"},\n",
        "            title=f\"Articles per Month Around {selected_date.strftime('%Y-%m-%d')}\",\n",
        "        )\n",
        "        fig.update_layout(xaxis_tickangle=-45)\n",
        "        # fig.show()\n",
        "        return fig\n",
        "        # return monthly\n",
        "\n",
        "    def plot_trend(self, freq=\"D\"):\n",
        "        if freq == \"D\":\n",
        "            counts = self.articles_per_day()\n",
        "        elif freq == \"W\":\n",
        "            counts = self.articles_per_week()\n",
        "        else:\n",
        "            counts = self.articles_per_month()\n",
        "        df = counts.reset_index()\n",
        "        df.columns = [\"Date\", \"Article_Count\"]\n",
        "        if pd.api.types.is_period_dtype(df[\"Date\"]):\n",
        "            df[\"Date\"] = df[\"Date\"].astype(str)\n",
        "\n",
        "        fig = px.line(\n",
        "            df,\n",
        "            x=\"Date\",\n",
        "            y=\"Article_Count\",\n",
        "            markers=True,\n",
        "            line_shape=\"linear\",\n",
        "            title=f\"Article Frequency Trend ({freq})\",\n",
        "        )\n",
        "\n",
        "        # Highlight peak\n",
        "        peak_idx = df[\"Article_Count\"].idxmax()\n",
        "        fig.add_scatter(\n",
        "            x=[df.loc[peak_idx, \"Date\"]],\n",
        "            y=[df.loc[peak_idx, \"Article_Count\"]],\n",
        "            mode=\"markers+text\",\n",
        "            marker=dict(color=\"seagreen\", size=14),\n",
        "            text=[f\"Peak: {df.loc[peak_idx, 'Article_Count']}\"],\n",
        "            textposition=\"top center\",\n",
        "            showlegend=False,\n",
        "        )\n",
        "        fig.update_layout(xaxis_tickangle=-45)\n",
        "        # fig.show()\n",
        "        return fig\n",
        "\n",
        "    def plot_tag_coverage_over_time(\n",
        "        self, tag, date_col=\"date\", tags_col=\"Tags\", top_n_months=None, color=\"#009688\"\n",
        "    ):\n",
        "        tag_lower = tag.lower()\n",
        "        mask = self.df[tags_col].apply(\n",
        "            lambda tags: tag_lower in [str(t).lower() for t in tags]\n",
        "            if isinstance(tags, list)\n",
        "            else tag_lower in str(tags).lower()\n",
        "        )\n",
        "        tag_df = self.df[mask]\n",
        "        if tag_df.empty:\n",
        "            print(f'No articles found with tag \"{tag}\"')\n",
        "            return\n",
        "        monthly_counts = tag_df.groupby(tag_df[date_col].dt.to_period(\"M\"))[\n",
        "            \"Id\"\n",
        "        ].count()\n",
        "        if top_n_months:\n",
        "            monthly_counts = (\n",
        "                monthly_counts.sort_values(ascending=False)\n",
        "                .head(top_n_months)\n",
        "                .sort_index()\n",
        "            )\n",
        "        df_plot = monthly_counts.reset_index()\n",
        "        df_plot.columns = [\"Month\", \"Article_Count\"]\n",
        "        df_plot[\"Month\"] = df_plot[\"Month\"].astype(str)\n",
        "\n",
        "        fig = px.bar(\n",
        "            df_plot,\n",
        "            x=\"Month\",\n",
        "            y=\"Article_Count\",\n",
        "            color=\"Article_Count\",\n",
        "            color_continuous_scale=\"teal\",\n",
        "            title=f'Coverage of \"{tag}\" Over Time',\n",
        "        )\n",
        "        # Highlight peak\n",
        "        peak_idx = df_plot[\"Article_Count\"].idxmax()\n",
        "        fig.add_scatter(\n",
        "            x=[df_plot.loc[peak_idx, \"Month\"]],\n",
        "            y=[df_plot.loc[peak_idx, \"Article_Count\"]],\n",
        "            mode=\"markers+text\",\n",
        "            marker=dict(color=\"seagreen\", size=16, symbol=\"diamond\"),\n",
        "            text=[f\"Peak: {df_plot.loc[peak_idx, 'Article_Count']}\"],\n",
        "            textposition=\"top center\",\n",
        "            showlegend=False,\n",
        "        )\n",
        "        fig.update_layout(xaxis_tickangle=-45)\n",
        "        # fig.show()\n",
        "        return fig\n",
        "        # return monthly_counts\n",
        "\n",
        "    def tag_cooccurrence_matrix(self, tag_col=\"Tags\", top_n=20, plot_heatmap=True):\n",
        "        co_counter = Counter()\n",
        "        tag_freq = Counter()\n",
        "        for taglist in self.df[tag_col].dropna():\n",
        "            unique_tags = list(set(taglist))\n",
        "            tag_freq.update(unique_tags)\n",
        "            for tag_pair in combinations(sorted(unique_tags), 2):\n",
        "                co_counter[tag_pair] += 1\n",
        "\n",
        "        top_tags = [tag for tag, _ in tag_freq.most_common(top_n)]\n",
        "        matrix = pd.DataFrame(0, index=top_tags, columns=top_tags, dtype=int)\n",
        "        for (tag1, tag2), count in co_counter.items():\n",
        "            if tag1 in top_tags and tag2 in top_tags:\n",
        "                matrix.loc[tag1, tag2] = count\n",
        "                matrix.loc[tag2, tag1] = count\n",
        "\n",
        "        if plot_heatmap:\n",
        "            fig = go.Figure(\n",
        "                data=go.Heatmap(\n",
        "                    z=matrix.values,\n",
        "                    x=matrix.columns,\n",
        "                    y=matrix.index,\n",
        "                    colorscale=\"Viridis\",\n",
        "                    colorbar={\"title\": \"Co-occurrence\"},\n",
        "                )\n",
        "            )\n",
        "            fig.update_layout(\n",
        "                title=f\"Tag Co-occurrence Heatmap (Top {top_n} Tags)\",\n",
        "                xaxis_title=\"Tag\",\n",
        "                yaxis_title=\"Tag\",\n",
        "                height=80 + 36 * top_n,\n",
        "            )\n",
        "            # fig.show()\n",
        "            return fig\n",
        "\n",
        "        return matrix\n",
        "\n",
        "    def get_tag_month_matrix(self, tag_col=\"Tags\", date_col=\"date\", top_n=10):\n",
        "        \"\"\"\n",
        "        Returns a DataFrame: rows=month, cols=top N tags, values=article counts.\n",
        "        \"\"\"\n",
        "        # Flatten: for each article and tag, record (month, tag)\n",
        "        rows = []\n",
        "        for idx, row in self.df.iterrows():\n",
        "            # if pd.isna(row[tag_col]) or not row[tag_col]:\n",
        "            #     continue\n",
        "            month = pd.to_datetime(row[date_col]).to_period(\"M\")\n",
        "            for tag in set(row[tag_col]):  # dedupe just in case\n",
        "                rows.append({\"month\": month, \"tag\": tag})\n",
        "\n",
        "        tag_month_df = pd.DataFrame(rows)\n",
        "        # Count occurrences per (month, tag)\n",
        "        tag_month_counts = (\n",
        "            tag_month_df.groupby([\"month\", \"tag\"]).size().reset_index(name=\"count\")\n",
        "        )\n",
        "        # Get top N tags overall\n",
        "        top_tags = (\n",
        "            tag_month_counts.groupby(\"tag\")[\"count\"]\n",
        "            .sum()\n",
        "            .sort_values(ascending=False)\n",
        "            .head(top_n)\n",
        "            .index.tolist()\n",
        "        )\n",
        "        tag_month_counts = tag_month_counts[tag_month_counts[\"tag\"].isin(top_tags)]\n",
        "        # Pivot to month × tag table\n",
        "        matrix = (\n",
        "            tag_month_counts.pivot(index=\"month\", columns=\"tag\", values=\"count\")\n",
        "            .fillna(0)\n",
        "            .astype(int)\n",
        "        )\n",
        "        # Sort by time\n",
        "        matrix = matrix.sort_index()\n",
        "        return matrix\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_tag_temporal_shifts(matrix):\n",
        "        # matrix: index=time, columns=tags, values=counts\n",
        "        if isinstance(matrix.index, pd.PeriodIndex):\n",
        "            matrix.index = matrix.index.astype(str)\n",
        "\n",
        "        df_long = matrix.reset_index().melt(\n",
        "            id_vars=matrix.index.name or \"index\", var_name=\"Tag\", value_name=\"Count\"\n",
        "        )\n",
        "        time_col = matrix.index.name or \"index\"\n",
        "\n",
        "        fig = px.area(\n",
        "            df_long,\n",
        "            x=time_col,\n",
        "            y=\"Count\",\n",
        "            color=\"Tag\",\n",
        "            title=\"Temporal Topic Shifts (Top Tags)\",\n",
        "            labels={time_col: \"Month\", \"Count\": \"Article Count\"},\n",
        "        )\n",
        "        fig.update_layout(xaxis_tickangle=-45)\n",
        "        # fig.show()\n",
        "        return fig\n",
        "\n",
        "    def topic_emergence_decay(\n",
        "        self, tag_col=\"Tags\", date_col=\"date\", freq=\"M\", min_window_count=3\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Identify emerging and disappearing tags per time window (e.g., month).\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame\n",
        "            tag_col: name of the column with tag lists\n",
        "            date_col: date column (must be datetime)\n",
        "            freq: window size (\"M\" for month, \"W\" for week, etc.)\n",
        "            min_window_count: only consider tags appearing at least this many times in a window\n",
        "\n",
        "        Returns:\n",
        "            emergence_df: DataFrame with window, emergent_tags, decayed_tags\n",
        "        \"\"\"\n",
        "        # 1. Assign window\n",
        "        df = self.df.copy()\n",
        "        df[\"window\"] = pd.to_datetime(df[date_col]).dt.to_period(freq)\n",
        "\n",
        "        # 2. Get tags per window\n",
        "        window_tags = {}\n",
        "        for window, group in df.groupby(\"window\"):\n",
        "            tags = []\n",
        "            for taglist in group[tag_col]:\n",
        "                if isinstance(taglist, list):\n",
        "                    tags += taglist\n",
        "            tag_counts = pd.Series(tags).value_counts()\n",
        "            tags_set = set(tag_counts[tag_counts >= min_window_count].index)\n",
        "            window_tags[window] = tags_set\n",
        "\n",
        "        # 3. Compare window to previous/next\n",
        "        windows = sorted(window_tags)\n",
        "        results = []\n",
        "        for i, win in enumerate(windows):\n",
        "            current_tags = window_tags[win]\n",
        "            prev_tags = window_tags[windows[i - 1]] if i > 0 else set()\n",
        "            next_tags = window_tags[windows[i + 1]] if i < len(windows) - 1 else set()\n",
        "            emergent = current_tags - prev_tags\n",
        "            decayed = current_tags - next_tags  # Tags present now, gone next window\n",
        "            results.append(\n",
        "                {\n",
        "                    \"window\": win,\n",
        "                    \"emergent_tags\": sorted(list(emergent)),\n",
        "                    \"decayed_tags\": sorted(list(decayed)),\n",
        "                    \"n_emergent\": len(emergent),\n",
        "                    \"n_decayed\": len(decayed),\n",
        "                }\n",
        "            )\n",
        "        emergence_df = pd.DataFrame(results)\n",
        "        return emergence_df\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_topic_emergence_decay(emergence_df, window_col=\"window\"):\n",
        "        fig = go.Figure()\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=emergence_df[window_col].astype(str),\n",
        "                y=emergence_df[\"n_emergent\"],\n",
        "                mode=\"lines+markers\",\n",
        "                name=\"Emergent tags\",\n",
        "            )\n",
        "        )\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=emergence_df[window_col].astype(str),\n",
        "                y=emergence_df[\"n_decayed\"],\n",
        "                mode=\"lines+markers\",\n",
        "                name=\"Decayed tags\",\n",
        "            )\n",
        "        )\n",
        "        fig.update_layout(\n",
        "            title=\"Topic Emergence and Decay Over Time\",\n",
        "            xaxis_title=\"Time Window\",\n",
        "            yaxis_title=\"Number of Tags\",\n",
        "            legend_title_text=None,\n",
        "            xaxis_tickangle=-45,\n",
        "        )\n",
        "        # fig.show()\n",
        "\n",
        "        # Optional: print/annotate top emergent/decayed tags for recent windows\n",
        "        print(\"\\nRecent Emergent and Decayed Tags:\")\n",
        "        display_df = emergence_df[[\"window\", \"emergent_tags\", \"decayed_tags\"]].tail(6)\n",
        "        print(display_df.to_string(index=False))\n",
        "        return fig\n",
        "\n",
        "    def plot_article_velocity_agg(\n",
        "        self,\n",
        "        tag,\n",
        "        tag_col=\"Tags\",\n",
        "        date_col=\"date\",\n",
        "        freq=\"M\",\n",
        "        agg=\"mean\",\n",
        "        time_unit=\"days\",\n",
        "    ):\n",
        "        tag_lower = tag.lower()\n",
        "        mask = self.df[tag_col].apply(\n",
        "            lambda tags: tag_lower in [str(t).lower() for t in tags]\n",
        "            if isinstance(tags, list)\n",
        "            else False\n",
        "        )\n",
        "        tag_dates = self.df.loc[mask, date_col]\n",
        "        if tag_dates.empty:\n",
        "            print(f\"No articles found for tag '{tag}'.\")\n",
        "            return\n",
        "\n",
        "        dates_sorted = pd.to_datetime(tag_dates).sort_values()\n",
        "        deltas = dates_sorted.diff().dropna()\n",
        "        if time_unit == \"days\":\n",
        "            delta_vals = deltas.dt.total_seconds() / 86400\n",
        "            unit_str = \"Days\"\n",
        "        elif time_unit == \"hours\":\n",
        "            delta_vals = deltas.dt.total_seconds() / 3600\n",
        "            unit_str = \"Hours\"\n",
        "        else:\n",
        "            delta_vals = deltas.dt.total_seconds() / 60\n",
        "            unit_str = \"Minutes\"\n",
        "\n",
        "        velocity = 1 / delta_vals.replace(0, float(\"nan\"))\n",
        "        vel_df = pd.DataFrame({\"date\": dates_sorted.iloc[1:], \"velocity\": velocity})\n",
        "        vel_df[\"window\"] = vel_df[\"date\"].dt.to_period(freq)\n",
        "\n",
        "        if agg == \"mean\":\n",
        "            agg_vel = vel_df.groupby(\"window\")[\"velocity\"].mean()\n",
        "        elif agg == \"max\":\n",
        "            agg_vel = vel_df.groupby(\"window\")[\"velocity\"].max()\n",
        "        else:\n",
        "            raise ValueError(\"agg must be 'mean' or 'max'\")\n",
        "\n",
        "        df_plot = agg_vel.reset_index()\n",
        "        df_plot.columns = [\"Time_Window\", \"Velocity\"]\n",
        "        df_plot[\"Time_Window\"] = df_plot[\"Time_Window\"].astype(str)\n",
        "\n",
        "        fig = px.line(\n",
        "            df_plot,\n",
        "            x=\"Time_Window\",\n",
        "            y=\"Velocity\",\n",
        "            markers=True,\n",
        "            title=f\"{agg.capitalize()} Article Velocity for '{tag}' by {freq}\",\n",
        "            labels={\"Velocity\": f\"Velocity (1/{unit_str})\"},\n",
        "        )\n",
        "        # Highlight peak\n",
        "        peak_idx = df_plot[\"Velocity\"].idxmax()\n",
        "        fig.add_scatter(\n",
        "            x=[df_plot.loc[peak_idx, \"Time_Window\"]],\n",
        "            y=[df_plot.loc[peak_idx, \"Velocity\"]],\n",
        "            mode=\"markers+text\",\n",
        "            marker=dict(color=\"seagreen\", size=14, symbol=\"diamond\"),\n",
        "            text=[f\"Peak: {df_plot.loc[peak_idx, 'Velocity']:.2f}\"],\n",
        "            textposition=\"top center\",\n",
        "            showlegend=False,\n",
        "        )\n",
        "        fig.update_layout(xaxis_tickangle=-45)\n",
        "        # fig.show()\n",
        "        print(f\"{agg.capitalize()} velocity stats:\\n{agg_vel.describe()}\")\n",
        "        return fig\n",
        "\n",
        "        # return agg_vel\n",
        "\n",
        "    def event_coverage_lifespan(\n",
        "        self, tag, tag_col=\"tags_norm\", date_col=\"date\", freq=\"D\"\n",
        "    ):\n",
        "        \"\"\"\n",
        "        For a given tag, find first, peak, and last article appearance, plus lifespan.\n",
        "        Optionally, return/plot daily or weekly trend.\n",
        "        \"\"\"\n",
        "        tag_lower = tag.lower()\n",
        "        mask = self.df[tag_col].apply(\n",
        "            lambda tags: tag_lower in [str(t).lower() for t in tags]\n",
        "            if isinstance(tags, list)\n",
        "            else False\n",
        "        )\n",
        "        event_df = self.df.loc[mask].copy()\n",
        "        if event_df.empty:\n",
        "            print(f\"No articles found for tag '{tag}'.\")\n",
        "            return None\n",
        "\n",
        "        event_df[\"date\"] = pd.to_datetime(event_df[date_col])\n",
        "        grouped = event_df.groupby(event_df[\"date\"].dt.to_period(freq)).size()\n",
        "\n",
        "        first_appearance = event_df[\"date\"].min()\n",
        "        last_appearance = event_df[\"date\"].max()\n",
        "        peak_window = grouped.idxmax()\n",
        "        peak_count = grouped.max()\n",
        "        lifespan_days = (last_appearance - first_appearance).days\n",
        "\n",
        "        print(f\"Event/tag: '{tag}'\")\n",
        "        print(f\"First appearance: {first_appearance.strftime('%Y-%m-%d')}\")\n",
        "        print(f\"Peak window: {peak_window} with {peak_count} articles\")\n",
        "        print(f\"Last appearance: {last_appearance.strftime('%Y-%m-%d')}\")\n",
        "        print(f\"Lifespan: {lifespan_days} days ({lifespan_days // 7} weeks)\")\n",
        "        print(f\"Total articles: {len(event_df)}\")\n",
        "\n",
        "        # Optional: return for visualization\n",
        "        return grouped, first_appearance, peak_window, last_appearance\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_event_lifespan(\n",
        "        grouped, first_appearance, peak_window, last_appearance, freq=\"D\", tag=\"\"\n",
        "    ):\n",
        "        # grouped: pd.Series (index=window, values=counts)\n",
        "        x_labels = list(grouped.index.astype(str))\n",
        "        first_label = str(first_appearance.to_period(freq))\n",
        "        last_label = str(last_appearance.to_period(freq))\n",
        "        peak_label = str(peak_window)\n",
        "\n",
        "        base_colors = [\"#1976D2\"] * len(x_labels)\n",
        "        for i, label in enumerate(x_labels):\n",
        "            if label == first_label:\n",
        "                base_colors[i] = \"mediumseagreen\"\n",
        "            if label == peak_label:\n",
        "                base_colors[i] = \"goldenrod\"\n",
        "            if label == last_label:\n",
        "                base_colors[i] = \"slateblue\"\n",
        "\n",
        "        fig = go.Figure()\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=x_labels, y=grouped.values, marker_color=base_colors, showlegend=False\n",
        "            )\n",
        "        )\n",
        "        # Custom legend\n",
        "        legend_colors = [\n",
        "            (\"First Appearance\", \"mediumseagreen\"),\n",
        "            (\"Peak Coverage\", \"goldenrod\"),\n",
        "            (\"Last Appearance\", \"slateblue\"),\n",
        "        ]\n",
        "        for name, color in legend_colors:\n",
        "            fig.add_trace(go.Bar(x=[None], y=[None], marker_color=color, name=name))\n",
        "\n",
        "        fig.update_layout(\n",
        "            title=f\"Coverage Lifespan for '{tag}' ({freq})\",\n",
        "            xaxis_title=\"Date\",\n",
        "            yaxis_title=\"Article Count\",\n",
        "            xaxis_tickangle=-45,\n",
        "            barmode=\"group\",\n",
        "        )\n",
        "        # fig.show()\n",
        "        return fig\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "\n",
        "class TextAnalyzer:\n",
        "    def __init__(self, source_file, id_col=\"Id\", title_col=\"Title\"):\n",
        "        self.source_file = source_file\n",
        "        self.df = None\n",
        "        self._load_data()\n",
        "\n",
        "        self.id_col = id_col\n",
        "        self.title_col = title_col\n",
        "\n",
        "    def _load_data(self):\n",
        "        \"\"\"Load the JSONL metadata file into a DataFrame.\"\"\"\n",
        "        try:\n",
        "            self.df = pd.read_json(self.source_file, lines=True)\n",
        "            print(f\"✅ Loaded {len(self.df)} records from {self.source_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to load data: {e}\")\n",
        "            self.df = pd.DataFrame()  # Empty fallback\n",
        "\n",
        "    @staticmethod\n",
        "    def _calculate_word_count(df, text_col, new_col=\"n_words\"):\n",
        "        \"\"\"\n",
        "        Adds/updates a word count column for the specified text column.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): The DataFrame.\n",
        "            text_col (str): The name of the text column.\n",
        "            new_col (str): The name for the word count column (default: 'n_words').\n",
        "        Returns:\n",
        "            pd.DataFrame: The DataFrame with the new column.\n",
        "        \"\"\"\n",
        "        df[new_col] = df[text_col].apply(\n",
        "            lambda x: len(str(x).split()) if pd.notnull(x) else 0\n",
        "        )\n",
        "        return df\n",
        "\n",
        "    def length_stats(self, text_col):\n",
        "        TextAnalyzer._calculate_word_count(df=self.df, text_col=text_col)\n",
        "        print(\"Article Count:\", len(self.df))\n",
        "        print(\"Min length (words):\", self.df[\"n_words\"].min())\n",
        "        print(\"Max length (words):\", self.df[\"n_words\"].max())\n",
        "        print(\"Mean length (words):\", self.df[\"n_words\"].mean())\n",
        "        print(\"Median length (words):\", self.df[\"n_words\"].median())\n",
        "        print(\"10 shortest articles:\")\n",
        "        print(\n",
        "            self.df[[self.id_col, self.title_col, \"n_words\"]]\n",
        "            .sort_values(\"n_words\")\n",
        "            .head(10)\n",
        "        )\n",
        "        print(\"10 longest articles:\")\n",
        "        print(\n",
        "            self.df[[self.id_col, self.title_col, \"n_words\"]]\n",
        "            .sort_values(\"n_words\", ascending=False)\n",
        "            .head(10)\n",
        "        )\n",
        "\n",
        "    def length_hist(self, text_col, bins=30):\n",
        "        TextAnalyzer._calculate_word_count(df=self.df, text_col=text_col)\n",
        "        fig = px.histogram(\n",
        "            self.df,\n",
        "            x=\"n_words\",\n",
        "            nbins=bins,\n",
        "            title=\"Distribution of Article Lengths (in words)\",\n",
        "            labels={\"n_words\": \"Number of Words\"},\n",
        "            opacity=0.85,\n",
        "            color_discrete_sequence=[\"#1976D2\"],\n",
        "        )\n",
        "        fig.update_layout(\n",
        "            xaxis_title=\"Number of Words\", yaxis_title=\"Number of Articles\", bargap=0.07\n",
        "        )\n",
        "        # fig.show()\n",
        "        return fig\n",
        "\n",
        "    def most_common_words(self, text_col, n=30, ngram=1):\n",
        "        ngram_list = []\n",
        "        for text in self.df[text_col].dropna():\n",
        "            tokens = str(text).split()\n",
        "            if len(tokens) < ngram:\n",
        "                continue\n",
        "            ngrams = zip(*[tokens[i:] for i in range(ngram)])\n",
        "            ngram_list.extend([\" \".join(ng) for ng in ngrams])\n",
        "        ngram_freq = Counter(ngram_list)\n",
        "        print(f\"Top {n} {ngram}-grams:\")\n",
        "        for ngram_str, freq in ngram_freq.most_common(n):\n",
        "            print(f\"{ngram_str}: {freq}\")\n",
        "\n",
        "        # Plotly barplot\n",
        "        top_ngrams = ngram_freq.most_common(n)\n",
        "        df_plot = pd.DataFrame(top_ngrams, columns=[\"Ngram\", \"Frequency\"])\n",
        "        fig = px.bar(\n",
        "            df_plot,\n",
        "            x=\"Ngram\",\n",
        "            y=\"Frequency\",\n",
        "            title=f\"Top {n} Most Common {ngram}-grams\",\n",
        "            labels={\"Ngram\": f\"{ngram}-gram\", \"Frequency\": \"Frequency\"},\n",
        "            color=\"Frequency\",\n",
        "            color_continuous_scale=\"Teal\",\n",
        "        )\n",
        "        fig.update_layout(\n",
        "            xaxis_title=f\"{ngram}-gram\", yaxis_title=\"Frequency\", xaxis_tickangle=-45\n",
        "        )\n",
        "        # fig.show()\n",
        "        return fig\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d457608f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to your data file\n",
        "DATA_PATH = \"22-7-2025_eda_filtered_preprocessed_eng.jsonl\"\n",
        "\n",
        "analyzer = Analyzer(DATA_PATH)\n",
        "text_analyzer = TextAnalyzer(source_file=DATA_PATH, id_col=\"Id\", title_col=\"title_norm\")  # adjust col names as needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11ac9e39",
      "metadata": {},
      "outputs": [],
      "source": [
        "analyzer.view_df(5)\n",
        "analyzer.info()\n",
        "analyzer.shape()\n",
        "analyzer.missing_report()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9804435",
      "metadata": {},
      "source": [
        "## Tag Distribution\n",
        "Visualize the most common topics/tags across the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34ed3cf8",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = analyzer.plot_tag_counts(tags_col=\"tags_norm\", top_n=20)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f81083e8",
      "metadata": {},
      "source": [
        "## Article Trends Over Time\n",
        "View daily/weekly/monthly publishing trends.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c41c8b74",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = analyzer.plot_trend(freq=\"M\")  # \"D\", \"W\", \"M\"\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94cee2d0",
      "metadata": {},
      "source": [
        "## Article Frequency Around Key Dates\n",
        "Visualize article counts centered on a selected date.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b26397ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = analyzer.articles_per_month_around_date(selected_date=\"2024-06-20\", months_window=3)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "125db4b0",
      "metadata": {},
      "source": [
        "## Tag Coverage Over Time\n",
        "Track the frequency of a given tag across months.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "061fed66",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = analyzer.plot_tag_coverage_over_time(tag=\"Gaza\", tags_col=\"tags_norm\", top_n_months=6)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88abb008",
      "metadata": {},
      "source": [
        "## Tag Co-occurrence\n",
        "Visualize which tags commonly appear together.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a098258b",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = analyzer.tag_cooccurrence_matrix(tag_col=\"tags_norm\", top_n=15)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cce6f50",
      "metadata": {},
      "source": [
        "## Topic Shifts Over Time\n",
        "Track how top topics change monthly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eee97013",
      "metadata": {},
      "outputs": [],
      "source": [
        "matrix = analyzer.get_tag_month_matrix(tag_col=\"tags_norm\", date_col=\"date\", top_n=10)\n",
        "fig = Analyzer.plot_tag_temporal_shifts(matrix)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15e7e032",
      "metadata": {},
      "source": [
        "## Topic Emergence/Decay\n",
        "Which topics are appearing/disappearing in each window?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7e27810",
      "metadata": {},
      "outputs": [],
      "source": [
        "emergence_df = analyzer.topic_emergence_decay(tag_col=\"tags_norm\", date_col=\"date\", freq=\"M\", min_window_count=2)\n",
        "fig = Analyzer.plot_topic_emergence_decay(emergence_df)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69ea436c",
      "metadata": {},
      "source": [
        "## Article Velocity for a Tag\n",
        "How quickly do articles about a topic appear?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67f36f18",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = analyzer.plot_article_velocity_agg(\n",
        "    tag=\"Gaza\", tag_col=\"tags_norm\", date_col=\"date\", time_unit=\"days\", freq=\"M\"\n",
        ")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "151cf819",
      "metadata": {},
      "source": [
        "## Event Lifespan\n",
        "When does a topic first, peak, and last appear?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bd9e83d",
      "metadata": {},
      "outputs": [],
      "source": [
        "grouped, first, peak, last = analyzer.event_coverage_lifespan(\n",
        "    \"Gaza\", tag_col=\"tags_norm\", date_col=\"date\", freq=\"M\"\n",
        ")\n",
        "if grouped is not None:\n",
        "    fig = Analyzer.plot_event_lifespan(grouped, first, peak, last, freq=\"M\", tag=\"Gaza\")\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60db2d0b",
      "metadata": {},
      "source": [
        "## Article Length Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a973593",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = text_analyzer.length_hist(text_col=\"full_text_norm\", bins=50)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0fdd4f1",
      "metadata": {},
      "source": [
        "## Most Common N-Grams (Unigrams, Bigrams, Trigrams)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b5f56df",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = text_analyzer.most_common_words(text_col=\"full_text_norm\", n=20, ngram=2)\n",
        "fig.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
